---
layout: post
title: How to Choose Right Machine Learning Model?
tags: models
---

<!-- more -->

---
Table of content:

{: class="table-of-content"}
* TOC
{:toc}

---

## How to choose right machine learning models?
Understand assumptions and restrictions of each machine learning model will help you to get correct starting point.

Below table try to list down some famous models and their advantages / disadvantages.

### Models

Task (T): 

    1. C = Classification.

    2. R = Regression.

Learning Type (L):

    1. S = Supervised

    2. U = Unsupervised

    3. C = Clustering

    4. R = Reinforcement learning

Method (M):

    1. N = Non-parametric 

    2. P = Parameteric

Approach (A):
    
    1. G = Generative

    2. D = Discriminative

Hyperparameters:

In statistics hyperparameters are parameters of a prior distribution. It relates to the parameters of our model.

|T|L|M|A|Algorithm|Assumption / Description|Advantages|Disadvantages|Hyperparameters|Loss Function|
|---|---|---|---|---|---|---|---|---|---|
|R|S|P|D|Linear Regression|Baseline predictions|1. Simple to understand and explain. 2. It seldom overfits. 3. Using L1 & L2 regularization is effective in feature selection. 4. Fast to train.|1. sensitive to outliers. 2. You have to work hard to make it fit nonlinear functions.|weights and bias| $$L(y, \hat{y}) = \frac{1}{M}\sum_{i=1}^{M}(\hat{y_i}-y_i)^2$$, $$y_i = w^Tx_i+b $$|
|C|S|P|D|Logistic regression|Independent and irrelevant alternatives (IIA) or independent and identically distributed (i.i.d.) assumption. Ordering results by probability|1. Simple to understand and explain. 2. It seldom overfits. 3. Using L1 & L2 regularization is effective in feature selection. 4. The best algorithm for predicting probabilities of an event. 5. Fast to train| 1. Can suffer from outliers. 2. You have to work hard to make it fit nonlinear functions|   |   |
|C|S|P|G|Naive Bayes|random variables are independent and identically distributed (i.i.d. assumption). Assume that the value of a particular feature is independent of the value of any other feature, given the class variable.|1. Easy and fast to implement. 2. doesn’t require too much memory and can be used for online learning. 3. Easy to understand. 4. Takes into account prior knowledge|1. Strong and unrealistic feature independence assumptions. 2. Fails estimating rare occurrences. 3. Suffers from irrelevant features.|   |   |
|C|S|P|G|Hidden Markov Model|  |   |  |   |   |
|C|S|P|D|Linear-chain Conditional Random Field| | | |   |   |
|C/R|S|P|D|Random Forest|Apt at almost any machine learning problem|1. Can work in parallel. 2. May overfits (Depth of tree is a parameters to control overfits). 3. Automatically handles missing values. 4. No need to transform any variable. 5. Can be used by almost anyone with excellent results| 1. Difficult to interpret due to complex multiple tree structures. 2. Weaker on regression when estimating values at the extremities of the distribution of response values. 3. Biased in multiclass problems toward more frequent classes.|   |   |
|C/R|S|P|D|Gradient Boosting|1. Apt at almost any machine learning problem. 2. Search engines (solving the problem of learning to rank)| 1. It can approximate most nonlinear function. 2. Best in class predictor. 3. Automatically handles missing values. 4. No need to transform any variable|1. It can overfit if run for too many iterations. 2. Sensitive to noisy data and outliers. 3. Doesn’t work well without parameter tuning|
|C|S|P|D|Support Vector Machines|0. Most popular model before NN works. 1. Character recognition. 2. Image recognition. 3. Text classification.|1. Automatic nonlinear feature creation. 2. Can approximate complex nonlinear functions|1. Difficult to interpret when applying nonlinear kernels. 2. Suffers from too many examples, after 10,000 examples it starts taking too long to train|   |   |
|C/R|C|N|D|K-nearest Neighbors|the input consists of the k closest training examples in the feature space|1.Fast. 2. lazy training. 3.Can naturally handle extreme multiclass problems (like tagging text)|1. Slow and cumbersome in the predicting phase(Model has to carry with data) 2. Can fail to predict correctly due to the (the volume of the space increases so fast that the available data become sparse)|   |   |
|C/R|S|P|D|Neural Networks / MLP (Multiple Layer Perceptron)|data space re-project until converge |1. Can approximate any nonlinear function. 2. Robust to outliers. 3. Works only with a portion of the examples (the support vectors)|1. Very difficult to set up. 2. Difficult to tune because of too many parameters and you have also to decide the architecture of the network. 3. Difficult to interpret 4. Easy to overfit| the number of layers, the number of neurons in each layer, the learning rate, regularization, dropout rate, batch size |   |
|C|S|P|D|Perceptron|Binary Classifier,  Assume data is binary classifiable or If the training data is linearly separable, the algorithm stops in a finite number of steps.|||   |   |
|C|U|N|D|PCA|PCA is limited to re-expressing the data as a linear combination of its basis vectors to best express the data mean||   |   |







### Reference:

1. [Machine Learning For Dummies Cheat Sheet](https://www.dummies.com/programming/big-data/data-science/machine-learning-dummies-cheat-sheet/)

2. [Machine Learning Explained: Algorithms Are Your Friend](https://blog.dataiku.com/machine-learning-explained-algorithms-are-your-friend)

3. [Cheat Sheet of Machine Learning and Python (and Math) Cheat Sheets](https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)

4. [Linear Regression](https://www.cs.toronto.edu/~frossard/post/linear_regression/)

5. [Hidden Markov Model and Naive Bayes relationship](http://www.davidsbatista.net/blog/2017/11/11/HHM_and_Naive_Bayes/)

6. [Maximum Entropy Markov Models and Logistic Regression](http://www.davidsbatista.net/blog/2017/11/12/Maximum_Entropy_Markov_Model/)

7. [Conditional Random Fields for Sequence Prediction](http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/)

8. [From Naive Bayes to Linear-chain CRF](http://cnyah.com/2017/08/26/from-naive-bayes-to-linear-chain-CRF/)

![Comparison between Naive Bayes, Logistic Regression, HMM, and CRF](http://cnyah.com/2017/08/26/from-naive-bayes-to-linear-chain-CRF/transforms.png)